{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = YOLO(\"./Yolo_models/bottle_detection_model/weights/best.pt\")\n",
    "model2 = YOLO(\"./Yolo_models/Blood/weights/best.pt\")\n",
    "model3 = YOLO(\"./Yolo_models/License_Plate/weights/best.pt\")\n",
    "model4 = YOLO(\"./Yolo_models/Smoke/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./Inputs/bl.jpeg\"\n",
    "OUTPUT_DIR = \"./Outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video opened successfully.\n",
      "Video FPS: 30.0\n",
      "Processing at 10 FPS, skipping every 3 frames\n",
      "\n",
      "0: 640x640 (no detections), 226.3ms\n",
      "Speed: 5.9ms preprocess, 226.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 230.7ms\n",
      "Speed: 3.4ms preprocess, 230.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 258.2ms\n",
      "Speed: 4.5ms preprocess, 258.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 276.8ms\n",
      "Speed: 4.8ms preprocess, 276.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 214.4ms\n",
      "Speed: 3.9ms preprocess, 214.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 224.5ms\n",
      "Speed: 4.3ms preprocess, 224.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 227.2ms\n",
      "Speed: 3.5ms preprocess, 227.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 226.4ms\n",
      "Speed: 11.5ms preprocess, 226.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 258.2ms\n",
      "Speed: 3.8ms preprocess, 258.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 219.8ms\n",
      "Speed: 4.2ms preprocess, 219.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 199.1ms\n",
      "Speed: 3.3ms preprocess, 199.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 200.9ms\n",
      "Speed: 4.9ms preprocess, 200.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 221.3ms\n",
      "Speed: 3.5ms preprocess, 221.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 250.2ms\n",
      "Speed: 4.7ms preprocess, 250.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 276.3ms\n",
      "Speed: 4.5ms preprocess, 276.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 235.6ms\n",
      "Speed: 6.0ms preprocess, 235.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 231.0ms\n",
      "Speed: 4.2ms preprocess, 231.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 272.4ms\n",
      "Speed: 5.4ms preprocess, 272.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 276.6ms\n",
      "Speed: 3.8ms preprocess, 276.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 233.2ms\n",
      "Speed: 5.2ms preprocess, 233.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 283.5ms\n",
      "Speed: 4.8ms preprocess, 283.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 228.7ms\n",
      "Speed: 4.6ms preprocess, 228.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 269.7ms\n",
      "Speed: 4.6ms preprocess, 269.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 260.0ms\n",
      "Speed: 7.7ms preprocess, 260.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 248.2ms\n",
      "Speed: 4.2ms preprocess, 248.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 261.3ms\n",
      "Speed: 4.5ms preprocess, 261.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 225.8ms\n",
      "Speed: 4.7ms preprocess, 225.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 206.8ms\n",
      "Speed: 3.7ms preprocess, 206.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 214.5ms\n",
      "Speed: 4.8ms preprocess, 214.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 200.7ms\n",
      "Speed: 3.8ms preprocess, 200.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 231.9ms\n",
      "Speed: 4.0ms preprocess, 231.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 241.1ms\n",
      "Speed: 4.0ms preprocess, 241.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 254.7ms\n",
      "Speed: 5.5ms preprocess, 254.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Error: Failed to read frame 97.\n",
      "Output video saved to: ./Outputs/temp.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "model_path = './Yolo_models/Smoke/weights/best.pt'\n",
    "model = YOLO(model_path)\n",
    "\n",
    "video_path = './Inputs/temp.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video file.\")\n",
    "else:\n",
    "    print(\"Video opened successfully.\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print(f\"Video FPS: {fps}\")\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "output_video_path = './Outputs/temp.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "desired_fps = 10\n",
    "frame_interval = int(fps // desired_fps)\n",
    "\n",
    "print(f\"Processing at {desired_fps} FPS, skipping every {frame_interval} frames\")\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(f\"Error: Failed to read frame {frame_count}.\")\n",
    "        break\n",
    "\n",
    "    if frame_count % frame_interval != 0:\n",
    "        frame_count += 1\n",
    "        continue\n",
    "\n",
    "    resized_frame = cv2.resize(frame, (640, 640))\n",
    "\n",
    "    results = model.predict(source=resized_frame, conf=0.3)\n",
    "\n",
    "    result_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    disclaimer_text = \"Smoking is injurious to health\"\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 3  # Increase font scale for a bigger text size\n",
    "    text_size = cv2.getTextSize(disclaimer_text, font, font_scale, 2)[0]\n",
    "    text_width = text_size[0]\n",
    "    text_height = text_size[1]\n",
    "\n",
    "    text_x = (frame_width - text_width) // 2\n",
    "    text_y = frame_height - 20\n",
    "\n",
    "    cv2.putText(result_frame, disclaimer_text, (text_x, text_y), font, font_scale, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "\n",
    "    out.write(cv2.cvtColor(result_frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"Output video saved to: {output_video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing at 10 FPS, skipping every 3 frames\n",
      "\n",
      "0: 640x640 1 Violent, 113.1ms\n",
      "Speed: 3.4ms preprocess, 113.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Timestamp: 00:00:00.000, Class: Violent, X: 106, Y: 240, X2: 388, Y2: 640\n",
      "\n",
      "0: 640x640 1 Violent, 125.9ms\n",
      "Speed: 5.2ms preprocess, 125.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Timestamp: 00:00:00.099, Class: Violent, X: 99, Y: 232, X2: 421, Y2: 639\n",
      "\n",
      "0: 640x640 1 Violent, 128.1ms\n",
      "Speed: 4.1ms preprocess, 128.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Timestamp: 00:00:00.198, Class: Violent, X: 92, Y: 229, X2: 429, Y2: 640\n",
      "\n",
      "0: 640x640 1 Violent, 130.6ms\n",
      "Speed: 4.7ms preprocess, 130.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Timestamp: 00:00:00.297, Class: Violent, X: 84, Y: 191, X2: 485, Y2: 640\n",
      "\n",
      "0: 640x640 1 Violent, 139.6ms\n",
      "Speed: 5.8ms preprocess, 139.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Timestamp: 00:00:00.396, Class: Violent, X: 124, Y: 2, X2: 629, Y2: 633\n",
      "\n",
      "0: 640x640 (no detections), 109.5ms\n",
      "Speed: 3.7ms preprocess, 109.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 135.7ms\n",
      "Speed: 4.0ms preprocess, 135.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 152.1ms\n",
      "Speed: 7.7ms preprocess, 152.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 119.6ms\n",
      "Speed: 3.9ms preprocess, 119.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 113.1ms\n",
      "Speed: 4.1ms preprocess, 113.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 142.2ms\n",
      "Speed: 3.8ms preprocess, 142.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 124.5ms\n",
      "Speed: 5.1ms preprocess, 124.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 132.2ms\n",
      "Speed: 3.4ms preprocess, 132.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 145.6ms\n",
      "Speed: 5.6ms preprocess, 145.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 114.4ms\n",
      "Speed: 3.4ms preprocess, 114.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 125.4ms\n",
      "Speed: 3.4ms preprocess, 125.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 111.7ms\n",
      "Speed: 3.4ms preprocess, 111.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 105.9ms\n",
      "Speed: 3.2ms preprocess, 105.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 108.5ms\n",
      "Speed: 3.4ms preprocess, 108.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 111.4ms\n",
      "Speed: 3.8ms preprocess, 111.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 107.4ms\n",
      "Speed: 4.3ms preprocess, 107.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 107.9ms\n",
      "Speed: 3.3ms preprocess, 107.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 116.3ms\n",
      "Speed: 3.4ms preprocess, 116.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 121.6ms\n",
      "Speed: 5.1ms preprocess, 121.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 125.5ms\n",
      "Speed: 5.0ms preprocess, 125.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 128.5ms\n",
      "Speed: 3.8ms preprocess, 128.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 110.4ms\n",
      "Speed: 3.6ms preprocess, 110.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 106.5ms\n",
      "Speed: 3.7ms preprocess, 106.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 108.2ms\n",
      "Speed: 3.7ms preprocess, 108.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 107.1ms\n",
      "Speed: 3.4ms preprocess, 107.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 107.1ms\n",
      "Speed: 4.4ms preprocess, 107.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 109.7ms\n",
      "Speed: 3.4ms preprocess, 109.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 108.2ms\n",
      "Speed: 3.2ms preprocess, 108.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 110.6ms\n",
      "Speed: 3.2ms preprocess, 110.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 115.5ms\n",
      "Speed: 3.5ms preprocess, 115.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 121.8ms\n",
      "Speed: 3.6ms preprocess, 121.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 125.6ms\n",
      "Speed: 3.9ms preprocess, 125.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 123.5ms\n",
      "Speed: 3.8ms preprocess, 123.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "model_path = 'Yolo_models/Blood/weights/best.pt'\n",
    "\n",
    "model = YOLO(model_path)\n",
    "\n",
    "\n",
    "video_path = 'Inputs/te.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video file.\")\n",
    "    exit()\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "desired_fps = 10\n",
    "frame_interval = int(fps // desired_fps) \n",
    "\n",
    "print(f\"Processing at {desired_fps} FPS, skipping every {frame_interval} frames\")\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if frame_count % frame_interval != 0:\n",
    "        frame_count += 1\n",
    "        continue\n",
    "\n",
    "    resized_frame = cv2.resize(frame, (640, 640))\n",
    "\n",
    "    results = model.predict(source=resized_frame, conf=0.3)\n",
    "\n",
    "    if len(results) > 0 and len(results[0].boxes) > 0:\n",
    "        timestamp_seconds = frame_count / fps\n",
    "        timestamp_milliseconds = int((timestamp_seconds - int(timestamp_seconds)) * 1000)\n",
    "        timestamp = time.strftime('%H:%M:%S', time.gmtime(timestamp_seconds))\n",
    "\n",
    "        for box in results[0].boxes.xyxy:\n",
    "            x1, y1, x2, y2 = box.cpu().numpy().astype(int)\n",
    "\n",
    "            class_id = int(results[0].boxes.cls[0].item())\n",
    "            class_name = model.names[class_id] if class_id in model.names else \"Unknown\"\n",
    "\n",
    "            print(f\"Timestamp: {timestamp}.{timestamp_milliseconds:03d}, \"\n",
    "                  f\"Class: {class_name}, \"\n",
    "                  f\"X: {x1}, Y: {y1}, X2: {x2}, Y2: {y2}\")\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing at 10 FPS, skipping every 2 frames\n",
      "\n",
      "0: 640x640 1 person, 156.1ms\n",
      "Speed: 4.6ms preprocess, 156.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:00.000, Class: person, X1: 253, Y1: 100, X2: 518, Y2: 639\n",
      "\n",
      "0: 640x640 1 0, 123.5ms\n",
      "Speed: 3.1ms preprocess, 123.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:00.000, Class: 0, X1: 257, Y1: 104, X2: 518, Y2: 640\n",
      "\n",
      "0: 640x640 (no detections), 105.5ms\n",
      "Speed: 3.0ms preprocess, 105.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 109.3ms\n",
      "Speed: 2.9ms preprocess, 109.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:00.000, Class: 0, X1: 0, Y1: 0, X2: 636, Y2: 639\n",
      "\n",
      "0: 640x640 1 cigarette, 100.5ms\n",
      "Speed: 2.9ms preprocess, 100.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Cigarette, Timestamp: 00:00:00.000, Class: cigarette, X1: 273, Y1: 284, X2: 307, Y2: 346\n",
      "\n",
      "0: 640x640 1 person, 124.5ms\n",
      "Speed: 3.5ms preprocess, 124.5ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:00.083, Class: person, X1: 253, Y1: 95, X2: 519, Y2: 639\n",
      "\n",
      "0: 640x640 1 0, 132.0ms\n",
      "Speed: 3.6ms preprocess, 132.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:00.083, Class: 0, X1: 257, Y1: 103, X2: 518, Y2: 640\n",
      "\n",
      "0: 640x640 (no detections), 146.9ms\n",
      "Speed: 6.5ms preprocess, 146.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 156.3ms\n",
      "Speed: 4.8ms preprocess, 156.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:00.083, Class: 0, X1: 0, Y1: 0, X2: 636, Y2: 639\n",
      "\n",
      "0: 640x640 1 cigarette, 132.0ms\n",
      "Speed: 3.8ms preprocess, 132.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Cigarette, Timestamp: 00:00:00.083, Class: cigarette, X1: 273, Y1: 282, X2: 308, Y2: 347\n",
      "\n",
      "0: 640x640 1 person, 140.0ms\n",
      "Speed: 4.0ms preprocess, 140.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:00.166, Class: person, X1: 253, Y1: 94, X2: 520, Y2: 639\n",
      "\n",
      "0: 640x640 1 0, 141.3ms\n",
      "Speed: 4.0ms preprocess, 141.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:00.166, Class: 0, X1: 258, Y1: 103, X2: 519, Y2: 640\n",
      "\n",
      "0: 640x640 (no detections), 162.5ms\n",
      "Speed: 5.3ms preprocess, 162.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 148.7ms\n",
      "Speed: 6.2ms preprocess, 148.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:00.166, Class: 0, X1: 0, Y1: 0, X2: 636, Y2: 639\n",
      "\n",
      "0: 640x640 1 cigarette, 117.0ms\n",
      "Speed: 3.8ms preprocess, 117.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Cigarette, Timestamp: 00:00:00.166, Class: cigarette, X1: 274, Y1: 282, X2: 309, Y2: 347\n",
      "\n",
      "0: 640x640 1 person, 117.7ms\n",
      "Speed: 3.3ms preprocess, 117.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:00.250, Class: person, X1: 253, Y1: 93, X2: 521, Y2: 639\n",
      "\n",
      "0: 640x640 1 0, 119.2ms\n",
      "Speed: 3.5ms preprocess, 119.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:00.250, Class: 0, X1: 258, Y1: 103, X2: 520, Y2: 640\n",
      "\n",
      "0: 640x640 (no detections), 122.2ms\n",
      "Speed: 4.5ms preprocess, 122.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 119.7ms\n",
      "Speed: 3.4ms preprocess, 119.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:00.250, Class: 0, X1: 0, Y1: 0, X2: 636, Y2: 639\n",
      "\n",
      "0: 640x640 1 cigarette, 126.3ms\n",
      "Speed: 3.7ms preprocess, 126.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Cigarette, Timestamp: 00:00:00.250, Class: cigarette, X1: 275, Y1: 283, X2: 311, Y2: 348\n",
      "\n",
      "0: 640x640 1 person, 144.6ms\n",
      "Speed: 4.4ms preprocess, 144.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:00.333, Class: person, X1: 253, Y1: 93, X2: 521, Y2: 639\n",
      "\n",
      "0: 640x640 1 0, 136.0ms\n",
      "Speed: 6.0ms preprocess, 136.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:00.333, Class: 0, X1: 258, Y1: 102, X2: 521, Y2: 640\n",
      "\n",
      "0: 640x640 (no detections), 128.4ms\n",
      "Speed: 5.0ms preprocess, 128.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 113.2ms\n",
      "Speed: 3.3ms preprocess, 113.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:00.333, Class: 0, X1: 0, Y1: 0, X2: 637, Y2: 639\n",
      "\n",
      "0: 640x640 1 cigarette, 117.1ms\n",
      "Speed: 3.3ms preprocess, 117.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Cigarette, Timestamp: 00:00:00.333, Class: cigarette, X1: 275, Y1: 283, X2: 313, Y2: 348\n",
      "\n",
      "0: 640x640 1 person, 121.7ms\n",
      "Speed: 3.8ms preprocess, 121.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:00.417, Class: person, X1: 259, Y1: 93, X2: 530, Y2: 638\n",
      "\n",
      "0: 640x640 1 0, 120.3ms\n",
      "Speed: 3.5ms preprocess, 120.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:00.417, Class: 0, X1: 260, Y1: 102, X2: 522, Y2: 640\n",
      "\n",
      "0: 640x640 (no detections), 126.4ms\n",
      "Speed: 4.1ms preprocess, 126.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 133.8ms\n",
      "Speed: 3.9ms preprocess, 133.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:00.417, Class: 0, X1: 0, Y1: 0, X2: 636, Y2: 639\n",
      "\n",
      "0: 640x640 1 cigarette, 125.5ms\n",
      "Speed: 3.5ms preprocess, 125.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Cigarette, Timestamp: 00:00:00.417, Class: cigarette, X1: 276, Y1: 275, X2: 312, Y2: 349\n",
      "\n",
      "0: 640x640 1 person, 145.0ms\n",
      "Speed: 3.8ms preprocess, 145.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:00.500, Class: person, X1: 259, Y1: 93, X2: 529, Y2: 638\n",
      "\n",
      "0: 640x640 1 0, 139.6ms\n",
      "Speed: 4.2ms preprocess, 139.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:00.500, Class: 0, X1: 260, Y1: 102, X2: 522, Y2: 640\n",
      "\n",
      "0: 640x640 (no detections), 128.6ms\n",
      "Speed: 3.7ms preprocess, 128.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 128.6ms\n",
      "Speed: 3.9ms preprocess, 128.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:00.500, Class: 0, X1: 0, Y1: 0, X2: 636, Y2: 639\n",
      "\n",
      "0: 640x640 1 cigarette, 116.8ms\n",
      "Speed: 4.0ms preprocess, 116.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Cigarette, Timestamp: 00:00:00.500, Class: cigarette, X1: 276, Y1: 271, X2: 312, Y2: 350\n",
      "\n",
      "0: 640x640 1 person, 124.4ms\n",
      "Speed: 3.3ms preprocess, 124.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:00.583, Class: person, X1: 261, Y1: 93, X2: 529, Y2: 638\n",
      "\n",
      "0: 640x640 1 0, 115.4ms\n",
      "Speed: 3.1ms preprocess, 115.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:00.583, Class: 0, X1: 260, Y1: 102, X2: 523, Y2: 640\n",
      "\n",
      "0: 640x640 (no detections), 113.7ms\n",
      "Speed: 3.9ms preprocess, 113.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 120.9ms\n",
      "Speed: 4.3ms preprocess, 120.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cigarette, 123.8ms\n",
      "Speed: 4.1ms preprocess, 123.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Cigarette, Timestamp: 00:00:00.583, Class: cigarette, X1: 277, Y1: 281, X2: 315, Y2: 350\n",
      "\n",
      "0: 640x640 1 person, 132.9ms\n",
      "Speed: 3.5ms preprocess, 132.9ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:00.667, Class: person, X1: 261, Y1: 92, X2: 528, Y2: 638\n",
      "\n",
      "0: 640x640 1 0, 135.4ms\n",
      "Speed: 6.4ms preprocess, 135.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:00.667, Class: 0, X1: 260, Y1: 102, X2: 523, Y2: 640\n",
      "\n",
      "0: 640x640 (no detections), 137.8ms\n",
      "Speed: 4.5ms preprocess, 137.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 139.4ms\n",
      "Speed: 4.3ms preprocess, 139.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cigarette, 145.4ms\n",
      "Speed: 5.6ms preprocess, 145.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Cigarette, Timestamp: 00:00:00.667, Class: cigarette, X1: 279, Y1: 282, X2: 315, Y2: 350\n",
      "\n",
      "0: 640x640 1 person, 125.5ms\n",
      "Speed: 3.8ms preprocess, 125.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:00.750, Class: person, X1: 261, Y1: 92, X2: 528, Y2: 638\n",
      "\n",
      "0: 640x640 1 0, 136.2ms\n",
      "Speed: 5.6ms preprocess, 136.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:00.750, Class: 0, X1: 260, Y1: 102, X2: 523, Y2: 640\n",
      "\n",
      "0: 640x640 (no detections), 119.5ms\n",
      "Speed: 6.5ms preprocess, 119.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 122.8ms\n",
      "Speed: 4.2ms preprocess, 122.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cigarette, 131.0ms\n",
      "Speed: 4.2ms preprocess, 131.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Cigarette, Timestamp: 00:00:00.750, Class: cigarette, X1: 280, Y1: 282, X2: 313, Y2: 349\n",
      "\n",
      "0: 640x640 1 person, 176.2ms\n",
      "Speed: 3.7ms preprocess, 176.2ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:00.834, Class: person, X1: 255, Y1: 94, X2: 523, Y2: 639\n",
      "\n",
      "0: 640x640 1 0, 154.7ms\n",
      "Speed: 4.9ms preprocess, 154.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:00.834, Class: 0, X1: 260, Y1: 101, X2: 523, Y2: 640\n",
      "\n",
      "0: 640x640 (no detections), 135.5ms\n",
      "Speed: 5.8ms preprocess, 135.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 137.7ms\n",
      "Speed: 5.5ms preprocess, 137.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cigarette, 127.3ms\n",
      "Speed: 4.2ms preprocess, 127.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Cigarette, Timestamp: 00:00:00.834, Class: cigarette, X1: 280, Y1: 281, X2: 313, Y2: 349\n",
      "\n",
      "0: 640x640 1 person, 141.1ms\n",
      "Speed: 4.1ms preprocess, 141.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:00.917, Class: person, X1: 255, Y1: 93, X2: 523, Y2: 639\n",
      "\n",
      "0: 640x640 1 0, 124.3ms\n",
      "Speed: 4.9ms preprocess, 124.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:00.917, Class: 0, X1: 260, Y1: 101, X2: 523, Y2: 640\n",
      "\n",
      "0: 640x640 (no detections), 114.6ms\n",
      "Speed: 3.2ms preprocess, 114.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 144.3ms\n",
      "Speed: 3.5ms preprocess, 144.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cigarette, 137.7ms\n",
      "Speed: 6.5ms preprocess, 137.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Cigarette, Timestamp: 00:00:00.917, Class: cigarette, X1: 278, Y1: 282, X2: 315, Y2: 350\n",
      "\n",
      "0: 640x640 1 person, 72.7ms\n",
      "Speed: 2.4ms preprocess, 72.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:01.001, Class: person, X1: 255, Y1: 93, X2: 524, Y2: 639\n",
      "\n",
      "0: 640x640 1 0, 69.1ms\n",
      "Speed: 2.5ms preprocess, 69.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:01.001, Class: 0, X1: 260, Y1: 100, X2: 522, Y2: 640\n",
      "\n",
      "0: 640x640 (no detections), 70.5ms\n",
      "Speed: 2.3ms preprocess, 70.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 76.1ms\n",
      "Speed: 3.0ms preprocess, 76.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cigarette, 73.9ms\n",
      "Speed: 2.3ms preprocess, 73.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Cigarette, Timestamp: 00:00:01.001, Class: cigarette, X1: 277, Y1: 281, X2: 315, Y2: 349\n",
      "\n",
      "0: 640x640 1 person, 79.5ms\n",
      "Speed: 2.2ms preprocess, 79.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:01.084, Class: person, X1: 256, Y1: 93, X2: 522, Y2: 639\n",
      "\n",
      "0: 640x640 1 0, 86.8ms\n",
      "Speed: 2.3ms preprocess, 86.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:01.084, Class: 0, X1: 259, Y1: 97, X2: 524, Y2: 639\n",
      "\n",
      "0: 640x640 (no detections), 76.4ms\n",
      "Speed: 2.3ms preprocess, 76.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 70.7ms\n",
      "Speed: 2.4ms preprocess, 70.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cigarette, 74.2ms\n",
      "Speed: 2.6ms preprocess, 74.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Cigarette, Timestamp: 00:00:01.084, Class: cigarette, X1: 278, Y1: 281, X2: 315, Y2: 350\n",
      "\n",
      "0: 640x640 1 person, 78.0ms\n",
      "Speed: 2.5ms preprocess, 78.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:01.167, Class: person, X1: 256, Y1: 93, X2: 521, Y2: 639\n",
      "\n",
      "0: 640x640 1 0, 75.7ms\n",
      "Speed: 2.9ms preprocess, 75.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:01.167, Class: 0, X1: 260, Y1: 98, X2: 522, Y2: 639\n",
      "\n",
      "0: 640x640 (no detections), 78.8ms\n",
      "Speed: 2.1ms preprocess, 78.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 74.8ms\n",
      "Speed: 2.1ms preprocess, 74.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:01.167, Class: 0, X1: 1, Y1: 0, X2: 636, Y2: 639\n",
      "\n",
      "0: 640x640 1 cigarette, 74.7ms\n",
      "Speed: 2.2ms preprocess, 74.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Cigarette, Timestamp: 00:00:01.167, Class: cigarette, X1: 277, Y1: 276, X2: 314, Y2: 351\n",
      "\n",
      "0: 640x640 1 person, 73.1ms\n",
      "Speed: 2.1ms preprocess, 73.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:01.251, Class: person, X1: 255, Y1: 94, X2: 519, Y2: 639\n",
      "\n",
      "0: 640x640 1 0, 66.9ms\n",
      "Speed: 2.8ms preprocess, 66.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:01.251, Class: 0, X1: 261, Y1: 98, X2: 520, Y2: 639\n",
      "\n",
      "0: 640x640 (no detections), 68.9ms\n",
      "Speed: 2.2ms preprocess, 68.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 71.0ms\n",
      "Speed: 3.5ms preprocess, 71.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:01.251, Class: 0, X1: 0, Y1: 0, X2: 636, Y2: 639\n",
      "\n",
      "0: 640x640 2 cigarettes, 70.4ms\n",
      "Speed: 2.1ms preprocess, 70.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Cigarette, Timestamp: 00:00:01.251, Class: cigarette, X1: 272, Y1: 277, X2: 320, Y2: 356\n",
      "Model: Cigarette, Timestamp: 00:00:01.251, Class: cigarette, X1: 273, Y1: 261, X2: 314, Y2: 351\n",
      "\n",
      "0: 640x640 1 person, 79.5ms\n",
      "Speed: 2.2ms preprocess, 79.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:01.334, Class: person, X1: 256, Y1: 93, X2: 519, Y2: 639\n",
      "\n",
      "0: 640x640 1 0, 76.0ms\n",
      "Speed: 2.2ms preprocess, 76.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:01.334, Class: 0, X1: 261, Y1: 100, X2: 481, Y2: 638\n",
      "\n",
      "0: 640x640 (no detections), 78.4ms\n",
      "Speed: 2.0ms preprocess, 78.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 69.0ms\n",
      "Speed: 2.1ms preprocess, 69.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:01.334, Class: 0, X1: 0, Y1: 1, X2: 637, Y2: 640\n",
      "\n",
      "0: 640x640 (no detections), 72.3ms\n",
      "Speed: 2.0ms preprocess, 72.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 toothbrush, 77.9ms\n",
      "Speed: 2.1ms preprocess, 77.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:01.418, Class: person, X1: 258, Y1: 93, X2: 527, Y2: 639\n",
      "Model: General, Timestamp: 00:00:01.418, Class: person, X1: 305, Y1: 284, X2: 360, Y2: 472\n",
      "\n",
      "0: 640x640 1 0, 68.4ms\n",
      "Speed: 2.2ms preprocess, 68.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:01.418, Class: 0, X1: 261, Y1: 98, X2: 522, Y2: 639\n",
      "\n",
      "0: 640x640 (no detections), 70.1ms\n",
      "Speed: 2.7ms preprocess, 70.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 69.7ms\n",
      "Speed: 2.2ms preprocess, 69.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:01.418, Class: 0, X1: 1, Y1: 0, X2: 638, Y2: 639\n",
      "\n",
      "0: 640x640 (no detections), 72.1ms\n",
      "Speed: 2.3ms preprocess, 72.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 74.1ms\n",
      "Speed: 2.1ms preprocess, 74.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:01.501, Class: person, X1: 259, Y1: 89, X2: 525, Y2: 638\n",
      "\n",
      "0: 640x640 1 0, 74.6ms\n",
      "Speed: 2.6ms preprocess, 74.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:01.501, Class: 0, X1: 261, Y1: 98, X2: 519, Y2: 639\n",
      "\n",
      "0: 640x640 (no detections), 68.8ms\n",
      "Speed: 2.7ms preprocess, 68.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 68.9ms\n",
      "Speed: 2.4ms preprocess, 68.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:01.501, Class: 0, X1: 2, Y1: 1, X2: 638, Y2: 640\n",
      "\n",
      "0: 640x640 (no detections), 68.4ms\n",
      "Speed: 2.5ms preprocess, 68.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 75.3ms\n",
      "Speed: 2.1ms preprocess, 75.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:01.584, Class: person, X1: 257, Y1: 81, X2: 539, Y2: 638\n",
      "\n",
      "0: 640x640 1 0, 74.8ms\n",
      "Speed: 2.5ms preprocess, 74.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:01.584, Class: 0, X1: 261, Y1: 98, X2: 519, Y2: 639\n",
      "\n",
      "0: 640x640 (no detections), 75.3ms\n",
      "Speed: 2.7ms preprocess, 75.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 72.9ms\n",
      "Speed: 2.1ms preprocess, 72.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:01.584, Class: 0, X1: 1, Y1: 1, X2: 638, Y2: 640\n",
      "\n",
      "0: 640x640 (no detections), 72.0ms\n",
      "Speed: 2.2ms preprocess, 72.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 74.8ms\n",
      "Speed: 2.1ms preprocess, 74.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:01.668, Class: person, X1: 256, Y1: 78, X2: 523, Y2: 638\n",
      "\n",
      "0: 640x640 1 0, 69.0ms\n",
      "Speed: 2.1ms preprocess, 69.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:01.668, Class: 0, X1: 260, Y1: 81, X2: 519, Y2: 638\n",
      "\n",
      "0: 640x640 (no detections), 71.8ms\n",
      "Speed: 2.1ms preprocess, 71.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 68.3ms\n",
      "Speed: 2.7ms preprocess, 68.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:01.668, Class: 0, X1: 1, Y1: 0, X2: 639, Y2: 638\n",
      "\n",
      "0: 640x640 (no detections), 72.5ms\n",
      "Speed: 2.8ms preprocess, 72.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 77.1ms\n",
      "Speed: 2.0ms preprocess, 77.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:01.751, Class: person, X1: 251, Y1: 67, X2: 601, Y2: 638\n",
      "\n",
      "0: 640x640 1 0, 73.5ms\n",
      "Speed: 2.5ms preprocess, 73.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:01.751, Class: 0, X1: 258, Y1: 65, X2: 519, Y2: 638\n",
      "\n",
      "0: 640x640 (no detections), 69.3ms\n",
      "Speed: 2.4ms preprocess, 69.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 83.9ms\n",
      "Speed: 2.1ms preprocess, 83.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:01.751, Class: 0, X1: 0, Y1: 0, X2: 638, Y2: 639\n",
      "\n",
      "0: 640x640 (no detections), 89.1ms\n",
      "Speed: 2.9ms preprocess, 89.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 89.2ms\n",
      "Speed: 3.1ms preprocess, 89.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 77.9ms\n",
      "Speed: 2.1ms preprocess, 77.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:01.835, Class: 0, X1: 256, Y1: 57, X2: 521, Y2: 638\n",
      "\n",
      "0: 640x640 (no detections), 78.1ms\n",
      "Speed: 2.7ms preprocess, 78.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 77.5ms\n",
      "Speed: 2.1ms preprocess, 77.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:01.835, Class: 0, X1: 0, Y1: 0, X2: 638, Y2: 638\n",
      "\n",
      "0: 640x640 (no detections), 74.9ms\n",
      "Speed: 2.6ms preprocess, 74.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 76.2ms\n",
      "Speed: 2.1ms preprocess, 76.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 76.1ms\n",
      "Speed: 2.2ms preprocess, 76.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:01.918, Class: 0, X1: 249, Y1: 50, X2: 525, Y2: 639\n",
      "\n",
      "0: 640x640 (no detections), 73.6ms\n",
      "Speed: 1.9ms preprocess, 73.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 78.5ms\n",
      "Speed: 3.3ms preprocess, 78.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:01.918, Class: 0, X1: 0, Y1: 0, X2: 639, Y2: 638\n",
      "\n",
      "0: 640x640 (no detections), 85.5ms\n",
      "Speed: 2.8ms preprocess, 85.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 92.6ms\n",
      "Speed: 4.0ms preprocess, 92.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 91.9ms\n",
      "Speed: 3.6ms preprocess, 91.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:02.002, Class: 0, X1: 253, Y1: 47, X2: 527, Y2: 639\n",
      "\n",
      "0: 640x640 (no detections), 90.4ms\n",
      "Speed: 2.8ms preprocess, 90.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 84.8ms\n",
      "Speed: 2.6ms preprocess, 84.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:02.002, Class: 0, X1: 0, Y1: 0, X2: 639, Y2: 638\n",
      "\n",
      "0: 640x640 (no detections), 88.3ms\n",
      "Speed: 2.7ms preprocess, 88.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 104.3ms\n",
      "Speed: 2.8ms preprocess, 104.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 100.5ms\n",
      "Speed: 3.2ms preprocess, 100.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:02.085, Class: 0, X1: 251, Y1: 48, X2: 529, Y2: 639\n",
      "\n",
      "0: 640x640 (no detections), 100.6ms\n",
      "Speed: 3.0ms preprocess, 100.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 96.1ms\n",
      "Speed: 3.0ms preprocess, 96.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:02.085, Class: 0, X1: 0, Y1: 0, X2: 639, Y2: 638\n",
      "\n",
      "0: 640x640 (no detections), 94.4ms\n",
      "Speed: 3.2ms preprocess, 94.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 toothbrush, 95.0ms\n",
      "Speed: 3.1ms preprocess, 95.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:02.168, Class: toothbrush, X1: 347, Y1: 84, X2: 521, Y2: 634\n",
      "Model: General, Timestamp: 00:00:02.168, Class: toothbrush, X1: 257, Y1: 58, X2: 520, Y2: 638\n",
      "\n",
      "0: 640x640 1 0, 87.6ms\n",
      "Speed: 3.1ms preprocess, 87.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:02.168, Class: 0, X1: 252, Y1: 51, X2: 531, Y2: 639\n",
      "\n",
      "0: 640x640 (no detections), 92.3ms\n",
      "Speed: 2.9ms preprocess, 92.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 93.3ms\n",
      "Speed: 2.6ms preprocess, 93.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:02.168, Class: 0, X1: 0, Y1: 0, X2: 638, Y2: 638\n",
      "\n",
      "0: 640x640 (no detections), 90.9ms\n",
      "Speed: 2.7ms preprocess, 90.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 96.9ms\n",
      "Speed: 2.8ms preprocess, 96.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:02.252, Class: person, X1: 258, Y1: 63, X2: 519, Y2: 638\n",
      "\n",
      "0: 640x640 1 0, 97.3ms\n",
      "Speed: 3.1ms preprocess, 97.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:02.252, Class: 0, X1: 255, Y1: 66, X2: 532, Y2: 639\n",
      "\n",
      "0: 640x640 (no detections), 98.0ms\n",
      "Speed: 3.9ms preprocess, 98.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 100.1ms\n",
      "Speed: 3.2ms preprocess, 100.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: License_Plate, Timestamp: 00:00:02.252, Class: 0, X1: 0, Y1: 0, X2: 638, Y2: 638\n",
      "\n",
      "0: 640x640 (no detections), 104.7ms\n",
      "Speed: 4.3ms preprocess, 104.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 103.6ms\n",
      "Speed: 3.8ms preprocess, 103.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: General, Timestamp: 00:00:02.335, Class: person, X1: 259, Y1: 66, X2: 518, Y2: 638\n",
      "\n",
      "0: 640x640 1 0, 97.7ms\n",
      "Speed: 3.2ms preprocess, 97.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Model: Bottle, Timestamp: 00:00:02.335, Class: 0, X1: 257, Y1: 72, X2: 525, Y2: 638\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Inference with all models\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 52\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresized_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     55\u001b[0m         timestamp_seconds \u001b[38;5;241m=\u001b[39m frame_count \u001b[38;5;241m/\u001b[39m fps\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:550\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:214\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:323\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 323\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:171\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    166\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    167\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    170\u001b[0m )\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:568\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 568\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:114\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:132\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:153\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 153\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    154\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\head.py:70\u001b[0m, in \u001b[0;36mDetect.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_end2end(x)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnl):\n\u001b[1;32m---> 70\u001b[0m     x[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv3[i](x[i])), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:  \u001b[38;5;66;03m# Training path\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:91\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\FYP2\\Audio_model\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "# Load all models\n",
    "models = {\n",
    "    \"General\": YOLO(\"yolov8n.pt\"),\n",
    "    \"Bottle\": YOLO(\"./Yolo_models/bottle_detection_model/weights/best.pt\"),\n",
    "    \"Blood\": YOLO(\"./Yolo_models/Blood/weights/best.pt\"),\n",
    "    \"License_Plate\": YOLO(\"./Yolo_models/License_Plate/weights/best.pt\"),\n",
    "    \"Cigarette\": YOLO(\"./Yolo_models/Smoke/weights/best.pt\")\n",
    "}\n",
    "\n",
    "# Path to the video\n",
    "video_path = 'Inputs/smoke1.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video file.\")\n",
    "    exit()\n",
    "\n",
    "# Get video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Set desired FPS\n",
    "desired_fps = 10\n",
    "frame_interval = int(fps // desired_fps)\n",
    "\n",
    "print(f\"Processing at {desired_fps} FPS, skipping every {frame_interval} frames\")\n",
    "\n",
    "# List to store detection results\n",
    "detections = []\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "# YOLO Inference loop\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if frame_count % frame_interval != 0:\n",
    "        frame_count += 1\n",
    "        continue\n",
    "\n",
    "    resized_frame = cv2.resize(frame, (640, 640))\n",
    "\n",
    "    # Inference with all models\n",
    "    for model_name, model in models.items():\n",
    "        results = model.predict(source=resized_frame, conf=0.3)\n",
    "\n",
    "        if len(results) > 0 and len(results[0].boxes) > 0:\n",
    "            timestamp_seconds = frame_count / fps\n",
    "            timestamp_milliseconds = int((timestamp_seconds - int(timestamp_seconds)) * 1000)\n",
    "            \n",
    "            # Proper timestamp formatting with milliseconds\n",
    "            timestamp = time.strftime('%H:%M:%S', time.gmtime(int(timestamp_seconds)))\n",
    "            timestamp_with_ms = f\"{timestamp}.{timestamp_milliseconds:03d}\"\n",
    "\n",
    "            for box in results[0].boxes.xyxy:\n",
    "                x1, y1, x2, y2 = box.cpu().numpy().astype(int)\n",
    "\n",
    "                # Get class ID and name\n",
    "                class_id = int(results[0].boxes.cls[0].item())\n",
    "                class_name = model.names[class_id] if class_id in model.names else \"Unknown\"\n",
    "\n",
    "                # Store the detection details in a list\n",
    "                detection = {\n",
    "                    \"model\": model_name,\n",
    "                    \"timestamp\": timestamp_with_ms,  # Including milliseconds\n",
    "                    \"class\": class_name,\n",
    "                    \"x1\": x1,\n",
    "                    \"y1\": y1,\n",
    "                    \"x2\": x2,\n",
    "                    \"y2\": y2\n",
    "                }\n",
    "                detections.append(detection)\n",
    "\n",
    "                # Print the detection immediately\n",
    "                print(f\"Model: {detection['model']}, \"\n",
    "                      f\"Timestamp: {detection['timestamp']}, \"\n",
    "                      f\"Class: {detection['class']}, \"\n",
    "                      f\"X1: {detection['x1']}, Y1: {detection['y1']}, \"\n",
    "                      f\"X2: {detection['x2']}, Y2: {detection['y2']}\")\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print the final list of detections\n",
    "print(\"\\nFinal Detection Results:\")\n",
    "for det in detections:\n",
    "    print(det)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(detections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 motorcycle, 130.6ms\n",
      "Speed: 6.6ms preprocess, 130.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 0, 127.5ms\n",
      "Speed: 3.1ms preprocess, 127.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Violent, 113.4ms\n",
      "Speed: 3.0ms preprocess, 113.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 122.8ms\n",
      "Speed: 3.7ms preprocess, 122.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 109.1ms\n",
      "Speed: 3.4ms preprocess, 109.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Detection Results:\n",
      "{'model': 'General', 'class': 'motorcycle', 'x1': 27, 'y1': 63, 'x2': 620, 'y2': 559}\n",
      "{'model': 'Bottle', 'class': '0', 'x1': 13, 'y1': 35, 'x2': 627, 'y2': 583}\n",
      "{'model': 'Blood', 'class': 'Violent', 'x1': 17, 'y1': 110, 'x2': 610, 'y2': 465}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load all models\n",
    "models = {\n",
    "    \"General\": YOLO(\"yolov8n.pt\"),\n",
    "    \"Bottle\": YOLO(\"./Yolo_models/bottle_detection_model/weights/best.pt\"),\n",
    "    \"Blood\": YOLO(\"./Yolo_models/Blood/weights/best.pt\"),\n",
    "    \"License_Plate\": YOLO(\"./Yolo_models/License_Plate/weights/best.pt\"),\n",
    "    \"Cigarette\": YOLO(\"./Yolo_models/Smoke/weights/best.pt\")\n",
    "}\n",
    "\n",
    "# Path to the image\n",
    "image_path = 'Inputs/spl.jpeg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "if image is None:\n",
    "    print(\"Error: Unable to load image.\")\n",
    "    exit()\n",
    "\n",
    "# Resize image for better inference\n",
    "resized_image = cv2.resize(image, (640, 640))\n",
    "\n",
    "# List to store detection results\n",
    "detections = []\n",
    "\n",
    "# Inference with all models\n",
    "for model_name, model in models.items():\n",
    "    results = model.predict(source=resized_image, conf=0.3)\n",
    "\n",
    "    if len(results) > 0 and len(results[0].boxes) > 0:\n",
    "        for box in results[0].boxes.xyxy:\n",
    "            x1, y1, x2, y2 = box.cpu().numpy().astype(int)\n",
    "\n",
    "            # Get class ID and name\n",
    "            class_id = int(results[0].boxes.cls[0].item())\n",
    "            class_name = model.names[class_id] if class_id in model.names else \"Unknown\"\n",
    "\n",
    "            # Store the detection details in a list\n",
    "            detection = {\n",
    "                \"model\": model_name,\n",
    "                \"class\": class_name,\n",
    "                \"x1\": x1,\n",
    "                \"y1\": y1,\n",
    "                \"x2\": x2,\n",
    "                \"y2\": y2\n",
    "            }\n",
    "            detections.append(detection)\n",
    "\n",
    "            # Draw bounding boxes and labels on the image\n",
    "            color = (0, 255, 0)  # Green box\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "            label = f\"{model_name}: {class_name}\"\n",
    "            cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "# Display the image with detections\n",
    "cv2.imshow(\"Detections\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print the detection results\n",
    "print(\"\\nDetection Results:\")\n",
    "for det in detections:\n",
    "    print(det)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
